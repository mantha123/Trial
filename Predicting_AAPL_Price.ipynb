{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPogpk9aZ/SlJ8CDro/cXr3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","Time-Series Forecasting: Predicting Apple Stock Price Using An LSTM Model\n","Traditionally, most machine learning (ML) models have used some observations (samples / examples) as input features, but the data has no time dimension.\n","\n","Models capable of predicting future values based on previously observed values are known as time-series forecasting models. For non-stationary data, time-series forecasting is widely used. Non-stationary data are those whose statistical features, such as mean and standard deviation, do not remain constant throughout time but instead change.\n","\n","The non-stationary input data used as input to these models is typically called to as time-series. Temperature values over time, stock prices over time, and the price of a property over time are all instances of time-series. As a result, the input is a signal (time-series) characterised by observations recorded in a specific order throughout time.\n","\n","The LSTM model\n","Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning\n","\n","LSTM Models, in a nutshell, can store data throughout time. When working with Time Series or Sequential Data, this is really handy.\n","\n"],"metadata":{"id":"n_FHNL46uNXO"}},{"cell_type":"code","source":["import pandas_datareader as web # to read data from web\n","import pandas as pd\n","import numpy as np\n"],"metadata":{"id":"4TfgBluFulUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the data\n","APPL_data= web.DataReader('AAPL',data_source=\"yahoo\",start='2015-01-01',end='2021-09-30')\n","APPL_data.head()"],"metadata":{"id":"dQMBSYjVu1Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["APPL_data.shape"],"metadata":{"id":"wgluRaFdvUXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lets look at the stock closing price of history\n","import seaborn as sns\n","plt.figure(figsize=(16,8))\n","sns.lineplot(x= APPL_data.index,y=APPL_data['Close'])\n","plt.xlabel('Date', fontsize=20)\n","plt.ylabel('closing price USD ($)',fontsize=20)\n","plt.title('Closing price History',fontsize=25)"],"metadata":{"id":"pfXc3HzBvWOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I am using the AAPL dataset, which is already divided into training set and test set, but you can do the divison with a simple command!"],"metadata":{"id":"pOoPjf7ZvlUs"}},{"cell_type":"code","source":["# Split into train and test:\n","data_to_train = APPL_data[:1530]\n","data_to_test = APPL_data[1530:]"],"metadata":{"id":"YDTNPIfVvml6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Now, we can save the 2 csv files, Train and Test.\n","data_to_train.to_csv('train_data.csv')\n","data_to_test.to_csv('test_data.csv')\n"],"metadata":{"id":"JWYB42aivr00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aapl_data= APPL_data.iloc[: , 3:4]\n","aapl_data.head()"],"metadata":{"id":"Hp0-MOmUvyq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## We want to create a numpy arrary not a vector\n","trainig_set= aapl_data.iloc[:1530,:].values\n","test_set= aapl_data.iloc[1530:,:].values"],"metadata":{"id":"lY7V_dYgvzWr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It’s a good idea to normalize the data before model fitting. This will boost the performance."],"metadata":{"id":"beuyiO3ywIxY"}},{"cell_type":"code","source":["# Feature scalling, Here we will do normalizatioin\n","from sklearn.preprocessing import MinMaxScaler\n","sc= MinMaxScaler(feature_range=(0,1))\n","trainig_set_scaled= sc.fit_transform(trainig_set)"],"metadata":{"id":"4z84j0VZwKK2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After scaling the training data, we must format it into a three-dimensional array for use in our LSTM model. We accomplish this by generating 60 timestamps."],"metadata":{"id":"tFa10wfKwb37"}},{"cell_type":"code","source":["# Create a data structure with 60 timesteps and 1 output\n","X_train=[] #Independent variables\n","y_train= [] # Dependent variables\n","# I am going to append past 60 days data\n","for i in range(60,1530):\n","    X_train.append(trainig_set_scaled[i-60:i,0]) # Appending prevois 60 days data not including 60\n","    y_train.append(trainig_set_scaled[i,0])\n","\n","X_train, y_train= np.array(X_train), np.array(y_train)"],"metadata":{"id":"ku1QtOxbwZjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lETS CHECK THE SHAPE OF X_train and y_train\n","X_train.shape, y_train.shape"],"metadata":{"id":"TqFw1MRQwj0F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have now reshaped the data into the following format (values, time-steps, 1 dimensional output)."],"metadata":{"id":"eL93jkgLwnah"}},{"cell_type":"code","source":["# LSMT Model needs to be 3- dimensional, so need to rehsape the x_train, y_train\n","# Reshaping\n","#numpy.reshape(array, shape, order = 'C')\n","X_train= np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n","X_train.shape"],"metadata":{"id":"JTpasBpHwkUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, it’s time to build the model. We will build the LSTM with 100 neurons and 5 hidden layers. Finally, we will assign 1 neuron in the output layer for predicting the normalized stock price. We will use the MSE loss function and the Adam stochastic gradient descent optimizer."],"metadata":{"id":"noepnOVIwv8W"}},{"cell_type":"code","source":["# Importing the Keras libraries and packages\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LSTM\n","\n","#  Initialising the RNN\n","model= Sequential()\n","\n","# Adding first LSTM layer and some dropout Dropout regularisation\n","model.add(LSTM(units=100,return_sequences=True, input_shape=(X_train.shape[1],1)))\n","model.add(Dropout(rate=0.2))\n","\n","# Adding second LSTM layer and some dropout Dropout regularisation\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(rate=0.2))\n","\n","# Adding third LSTM layer and some dropout Dropout regularisation\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(rate=0.2))\n","\n","# Adding fourth LSTM layer and some dropout Dropout regularisation\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(rate=0.2))\n","\n","# Adding fifth LSTM layer and some dropout Dropout regularisation\n","model.add(LSTM(units=100))\n","model.add(Dropout(rate=0.2))\n","\n","# Adding the Output Layer\n","model.add(Dense(units=1))\n","\n","# Compiling the Model\n","# Because we're doing regression hence mean_squared_error\n","model.compile(loss='mean_squared_error', optimizer='adam')"],"metadata":{"id":"IMAkXde1wr5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"raPZfKAyw3ze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use Dropout layers to avoid Overfitting problems, and besides that, we use the parameter “return_sequences” to determine if the layer will return a sequence compatible with a LSTM. We use “return_sequences=True” when we have a LSTM layer after"],"metadata":{"id":"VCUhRH5sw6az"}},{"cell_type":"code","source":["# Fitting the model to the Training set\n","history=model.fit(X_train,y_train,epochs=100,batch_size=32)"],"metadata":{"id":"27-wMLRKw4im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluating The Model\n","plt.figure(figsize=(15,6))\n","plt.plot(history.history['loss'])\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.show()"],"metadata":{"id":"-EWLNyT5xEr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To make predictions, we need to use our Test Dataset and format him like we did in the Train Dataset before."],"metadata":{"id":"Vp5iiUkExJK6"}},{"cell_type":"code","source":["#GEtting ready both train and est data set\n","train_data= pd.read_csv('train_data.csv')\n","test_data= pd.read_csv('test_data.csv')"],"metadata":{"id":"6tY6mavnxLId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_stock_price = test_data.iloc[:, 3:4].values"],"metadata":{"id":"yfl-uSSqxWVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_stock_price.shape"],"metadata":{"id":"iKyXwOcUxY4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set.shape"],"metadata":{"id":"QZ2IRAeXxa3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hence we will concatenate the dataset and then scale them\n","data_total= pd.concat([train_data['Close'], test_data['Close']],  axis=0)\n","inputs= data_total[len(data_total)-len(test_data)-60:].values\n","inputs = inputs.reshape(-1,1)\n","inputs = sc.transform(inputs)\n","\n","X_test = []\n","for i in range(60, 230):\n","    X_test.append(inputs[i-60:i, 0])\n","\n","X_test = np.array(X_test)\n","# 3D format\n","X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"],"metadata":{"id":"wm82yWiLxdKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs.shape"],"metadata":{"id":"rWzVSrCKxhCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_total.shape"],"metadata":{"id":"k9wbO1RXxjC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"id":"xRPvcTyvxk9j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After format the Test Data, we can make predictions in our X_test."],"metadata":{"id":"SbKTKpXKxp5I"}},{"cell_type":"code","source":["#preict the model\n","predicted_stock_price = model.predict(X_test)"],"metadata":{"id":"8fkQ2Jbhxr6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But before plot our predictions, we need to make a inverse_transform() in the predictions array, because we make predictions using the Scale, so our predictions are between 0 and 1."],"metadata":{"id":"gDalmhgsxx59"}},{"cell_type":"code","source":["# Inverse the scaling\n","predicted_stock_price = sc.inverse_transform(predicted_stock_price)"],"metadata":{"id":"sqE9m5fwxzIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualising the results\n","plt.figure(figsize=(15,8))\n","plt.plot(real_stock_price, color='Red', label='Real Apple Stock Price')\n","plt.plot(predicted_stock_price, color='Blue', label='Predicted Apple Stock Price')\n","plt.title('Apple Stock Price Prediction',fontsize=20)\n","plt.xlabel('Time', fontsize=15)\n","plt.ylabel('Apple Stock Price',fontsize=15)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Yx3s3k6ox1m7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualising the results\n","plt.figure(figsize=(15,8))\n","plt.plot(test_set, color='Red', label='Real Apple Stock Price')\n","plt.plot(predicted_stock_price, color='Blue', label='Predicted Apple Stock Price')\n","plt.title('Apple Stock Price Prediction')\n","plt.xlabel('Time')\n","plt.ylabel('Apple Stock Price')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"VQ44h0MNx3xd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our model performed admirably, as can be seen. It can accurately follow most unusual jumps/drops; however, we can observe that the model expected (predicted) lower values than the actual stock price for the most recent date stamps."],"metadata":{"id":"PmBKVAVWyCfY"}}]}